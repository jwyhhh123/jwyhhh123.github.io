<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Robotics:Robot sensors Color sensor Ir_Sensor Ultra_sonic_Sensor Touch_Sensor  Bobot Motors.pilot.chassis Arbitratorfirst assignment.Behavior Color Sensor.ColorIDMode.ColorRGBMode.AmbientMode Navigato">
<meta property="og:type" content="article">
<meta property="og:title" content="Log.AI">
<meta property="og:url" content="https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/index.html">
<meta property="og:site_name" content="WY J">
<meta property="og:description" content="Robotics:Robot sensors Color sensor Ir_Sensor Ultra_sonic_Sensor Touch_Sensor  Bobot Motors.pilot.chassis Arbitratorfirst assignment.Behavior Color Sensor.ColorIDMode.ColorRGBMode.AmbientMode Navigato">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-05-13T22:37:36.532Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Log.AI">
<meta name="twitter:description" content="Robotics:Robot sensors Color sensor Ir_Sensor Ultra_sonic_Sensor Touch_Sensor  Bobot Motors.pilot.chassis Arbitratorfirst assignment.Behavior Color Sensor.ColorIDMode.ColorRGBMode.AmbientMode Navigato">
    
    
        
          
              <link rel="shortcut icon" href="https://www.gravatar.com/avatar/0f8db8a65210b2d3b1aa9d7887cbeefd?s=16">
          
        
        
          
            <link rel="icon" type="image/png" href="https://www.gravatar.com/avatar/0f8db8a65210b2d3b1aa9d7887cbeefd?s=192" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="https://www.gravatar.com/avatar/0f8db8a65210b2d3b1aa9d7887cbeefd?s=180">
          
        
    
    <!-- title -->
    <title>Log.AI</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </span>
    <br>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/02/20/Log-DataStructure-1/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/02/19/Log-logic/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&text=Log.AI"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&is_video=false&description=Log.AI"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Log.AI&body=Check out this article: https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&name=Log.AI&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Robotics"><span class="toc-number">1.</span> <span class="toc-text">Robotics:</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Robot-sensors"><span class="toc-number">1.1.</span> <span class="toc-text">Robot sensors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bobot-Motors"><span class="toc-number">1.2.</span> <span class="toc-text">Bobot Motors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Arbitrator"><span class="toc-number">1.3.</span> <span class="toc-text">Arbitrator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Color-Sensor"><span class="toc-number">1.4.</span> <span class="toc-text">Color Sensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Navigator"><span class="toc-number">1.5.</span> <span class="toc-text">Navigator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mapping"><span class="toc-number">1.6.</span> <span class="toc-text">Mapping</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Robot-wifi-connection-amp-assignment"><span class="toc-number">1.7.</span> <span class="toc-text">Robot wifi connection &amp; assignment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bluetooth-connection"><span class="toc-number">1.8.</span> <span class="toc-text">Bluetooth connection</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Theory-Part"><span class="toc-number">2.</span> <span class="toc-text">Theory Part:</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Swarm-intelligence-and-firefly-synchronization"><span class="toc-number">2.1.</span> <span class="toc-text">Swarm intelligence and firefly synchronization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Uninformed-Search"><span class="toc-number">2.2.</span> <span class="toc-text">Uninformed Search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-star-Search"><span class="toc-number">2.3.</span> <span class="toc-text">A star Search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hill-Climbing"><span class="toc-number">2.4.</span> <span class="toc-text">Hill Climbing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Simulated-annealing-模拟退火"><span class="toc-number">2.5.</span> <span class="toc-text">Simulated annealing(模拟退火)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction-to-machine-learning-and-k-NN-聚类算法"><span class="toc-number">2.6.</span> <span class="toc-text">Introduction to machine learning and k-NN(聚类算法)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Naive-Bayes"><span class="toc-number">2.7.</span> <span class="toc-text">Naive Bayes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression"><span class="toc-number">2.8.</span> <span class="toc-text">Linear Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-regression"><span class="toc-number">2.9.</span> <span class="toc-text">Logistic regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Revisit"><span class="toc-number">2.10.</span> <span class="toc-text">Revisit</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Network"><span class="toc-number">2.11.</span> <span class="toc-text">Neural Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reinforcement-learning"><span class="toc-number">2.12.</span> <span class="toc-text">Reinforcement learning</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Log.AI
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">WY J</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-02-19T00:06:14.000Z" itemprop="datePublished">2019-02-19</time>
        
      
    </div>


      

      

    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Robotics"><a href="#Robotics" class="headerlink" title="Robotics:"></a>Robotics:</h1><h2 id="Robot-sensors"><a href="#Robot-sensors" class="headerlink" title="Robot sensors"></a>Robot sensors</h2><ul>
<li>Color sensor</li>
<li>Ir_Sensor</li>
<li>Ultra_sonic_Sensor</li>
<li>Touch_Sensor</li>
</ul>
<h2 id="Bobot-Motors"><a href="#Bobot-Motors" class="headerlink" title="Bobot Motors"></a>Bobot Motors</h2><p>.pilot<br>.chassis</p>
<h2 id="Arbitrator"><a href="#Arbitrator" class="headerlink" title="Arbitrator"></a>Arbitrator</h2><p>first assignment<br>.Behavior</p>
<h2 id="Color-Sensor"><a href="#Color-Sensor" class="headerlink" title="Color Sensor"></a>Color Sensor</h2><p>.ColorIDMode<br>.ColorRGBMode<br>.AmbientMode</p>
<h2 id="Navigator"><a href="#Navigator" class="headerlink" title="Navigator"></a>Navigator</h2><p>.Waypoint<br>.Heading</p>
<h2 id="Mapping"><a href="#Mapping" class="headerlink" title="Mapping"></a>Mapping</h2><ul>
<li>Topology map</li>
<li>metric map</li>
<li>cell-based map</li>
<li>occupancy grids: 0-empty; 1-occupied</li>
</ul>
<h2 id="Robot-wifi-connection-amp-assignment"><a href="#Robot-wifi-connection-amp-assignment" class="headerlink" title="Robot wifi connection &amp; assignment"></a>Robot wifi connection &amp; assignment</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> host A                 host B</span><br><span class="line">    |                     |</span><br><span class="line">server socket &lt;-------&gt; socket</span><br><span class="line">            |        </span><br><span class="line">          port1234</span><br><span class="line">  (server: robot)         (client: PC)</span><br></pre></td></tr></table></figure>
<h2 id="Bluetooth-connection"><a href="#Bluetooth-connection" class="headerlink" title="Bluetooth connection"></a>Bluetooth connection</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import java.io.*;</span><br><span class="line">import java.net.*;</span><br><span class="line">import lejos.hardware.Battery;</span><br><span class="line"></span><br><span class="line">public class EV3Server &#123;</span><br><span class="line">    public static final int port = 1234;</span><br><span class="line">    public static void main(String[] args) throws IOException &#123;</span><br><span class="line">        ServerSocket server = new ServerSocket(port);</span><br><span class="line">        System.out.println(&quot;Awaiting client..&quot;);</span><br><span class="line">        Socket client = server.accept();</span><br><span class="line">        System.out.println(&quot;CONNECTED&quot;);</span><br><span class="line">        OutputStream out = client.getOutputStream();</span><br><span class="line">        DataOutputStream dOut = new DataOutputStream(out);</span><br><span class="line">        dOut.writeUTF(&quot;Battery: &quot; + Battery.getVoltage());</span><br><span class="line">        dOut.flush();</span><br><span class="line">        server.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import java.io.*;</span><br><span class="line">import java.net.*;</span><br><span class="line"></span><br><span class="line">public class PCClient &#123;</span><br><span class="line">    public static void main(String[] args) throws IOException &#123;</span><br><span class="line">        String ip = &quot;10.0.1.1&quot;; // BT</span><br><span class="line">        if(args.length &gt; 0)ip = args[0];</span><br><span class="line">        Socket sock = new Socket(ip, EV3Server.port);</span><br><span class="line">        System.out.println(&quot;Connected&quot;);</span><br><span class="line">        InputStream in = sock.getInputStream();</span><br><span class="line">        DataInputStream dIn = new DataInputStream(in);</span><br><span class="line">        String str = dIn.readUTF();</span><br><span class="line">        System.out.println(str);</span><br><span class="line">        sock.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>boolean isGoal(State_goal)</p>
<h1 id="Theory-Part"><a href="#Theory-Part" class="headerlink" title="Theory Part:"></a>Theory Part:</h1><h2 id="Swarm-intelligence-and-firefly-synchronization"><a href="#Swarm-intelligence-and-firefly-synchronization" class="headerlink" title="Swarm intelligence and firefly synchronization"></a>Swarm intelligence and firefly synchronization</h2><p><strong>Swarm intelligence</strong>(群体智能)<br>features :</p>
<ol>
<li>decentrialized control</li>
<li>self-organization<ul>
<li>no central authority</li>
<li>local interaction pattern</li>
</ul>
</li>
</ol>
<p><strong>firefly synchronization</strong>(萤火虫同步)<br>representation :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--^----^----||----^----^----      actual flash     </span><br><span class="line">                  ^             expected flash</span><br><span class="line">^----^----^-||----^----^----</span><br><span class="line">                ^</span><br><span class="line">---^----^---||----^----^----</span><br><span class="line">              ^</span><br><span class="line">-^----^-----||^----^----^---</span><br><span class="line">             ^</span><br></pre></td></tr></table></figure>
<p>algorithm:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(firefly f: fireflys[][])&#123;</span><br><span class="line">    <span class="keyword">if</span>(isflashing(f.neighbour)) &#123;</span><br><span class="line">        <span class="keyword">if</span>(f.timer [<span class="number">0</span>:<span class="number">3</span>]) &#123;</span><br><span class="line">            tick++;</span><br><span class="line">            f.timer decrements by <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(f.timer [<span class="number">4</span>:<span class="number">7</span>]) &#123;</span><br><span class="line">            tick++;</span><br><span class="line">            f.timer increments by <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(f.timer == <span class="number">8</span> || f.timer == <span class="number">9</span>) &#123;</span><br><span class="line">             tick++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; </span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">        tick++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Advantages</strong><br>1) Scalability : when the program is running, we can still increase the size of our objects without affecting other “fireflies”.<br>2) Fault tolerance : the program can still work when some fireflies die or has error.<br>3) No need for a central system</p>
<p><strong>Applications</strong><br>wireless sensor networks: </p>
<ul>
<li>to enable object detection</li>
<li>to save battery</li>
</ul>
<p>robot fault detection</p>
<h2 id="Uninformed-Search"><a href="#Uninformed-Search" class="headerlink" title="Uninformed Search"></a>Uninformed Search</h2><p><strong>Problem Formulation</strong></p>
<ol>
<li>initial state</li>
<li>goal state</li>
<li>possible actions and their effects on states</li>
<li>cost function</li>
</ol>
<p><strong>Breadth First Search:</strong> Queue<br>Complete: guaranted to find a goal state if one exits and terminate if failed.<br>Admissible(可采纳的): able to find the shortest path.<br>Space.Compx = O(b^d - 1) = O(b^d)<br>Time.Compx = O(b^d -1) = O(b^d)<br><code>b</code>: branching factor (二叉 : 2 ; 三叉 : 3)<br><code>d</code>: depth of the goal</p>
<p><strong>Depth First Search:</strong> Stack<br>Complete: if the state space is finite,…(incomplete if infinite)<br>Not admissible: not guaranteed to find the shortest path to a goal<br>Space.compx = O(b^m)<br>Time.Compx = O(b^m)<br><code>b</code>: branching factor<br><code>m</code>: maximum depth of the search tree</p>
<p>NOTES: </p>
<ul>
<li>If a state can be reached by two nodes, we only remember the node associated to the lowest cost.</li>
<li>in real practice, it is not the case we can simply find the shortest path, we need to explore the entire maze before.</li>
<li>we mark explored nodes in a visited list, and nodes in the frontier is stored in an unvisited list.</li>
</ul>
<p><strong>Analysing Searching Algorithms</strong><br>Completeness     : if it is guaranteed to find a solution when at least one solution exists.<br>Admissibility    : if it is guaranteed to find the best solution when there are more than one solution.<br>Time Complexity  : the order of computation during the search process in the worst case<br>Space Complexity : storage space required in the worst case</p>
<p><strong>Web Src</strong><br><a href="https://zhuanlan.zhihu.com/p/29425290" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/29425290</a><br><a href="https://blog.csdn.net/dyy_gusi/article/details/46414677" target="_blank" rel="noopener">https://blog.csdn.net/dyy_gusi/article/details/46414677</a></p>
<h2 id="A-star-Search"><a href="#A-star-Search" class="headerlink" title="A star Search"></a>A star Search</h2><p><strong>A-star search</strong><br>def : A best-first informed search algorithm<br>&nbsp;<br>NOTE : Unlike BFS and DFS which ONLY take tree structure into consider, A-star searching algorithm is more like a mixture of BFS and DFS. It depends on a cost function to evaluate which node we should visit first.</p>
<p><strong>Evaluation function :</strong><br>f(node) = g(node) + h(node)<br><code>g(node)</code> : cost of the actions<br><code>h(node)</code> : heuristic function (从指定的方格移动到终点 B 的估算成本。这个通常被称为试探法)</p>
<p><strong>Algorithm :</strong></p>
<ol>
<li>visit the node with the smallest f(node) first, placing its children in the frontier.</li>
<li>do not place children in the frontier if their corresponding state is already in the frontier or in the list of visited nodes. （已经被估值的节点将不再计入, closedSet and openSet)</li>
<li>when found a goal state :<ul>
<li>continue process 2 if there exists node with lower cost</li>
<li>return goal state if there is no other nodes with smaller cost.</li>
</ul>
</li>
</ol>
<p>When heuristic h(x) is consistent, A-star is :</p>
<ul>
<li>Complete</li>
<li>admissible</li>
<li>optimally efficient</li>
</ul>
<p>Space.Compx = usually exponential<br>Time.Compx = usually exponential</p>
<p><strong>Generating heuristics</strong><br>Heuristic are problem dependent.<br>&nbsp;<br>NOTE : in a 8-tile puzzle problem, heuristic would be the sum of the Manhattan distance between each tile and its goal position. Or, it can be the sum of misplaced tiles.</p>
<p><strong>Web Rsc</strong><br><a href="https://zhuanlan.zhihu.com/p/38595351" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38595351</a></p>
<h2 id="Hill-Climbing"><a href="#Hill-Climbing" class="headerlink" title="Hill Climbing"></a>Hill Climbing</h2><p><strong>Examples of Optimisation Problems</strong></p>
<ul>
<li>Minimum distance route problem</li>
<li>Travelling Salesman Problem (TSP)</li>
<li>Bin Packing problem</li>
<li>Software Module Clustering</li>
</ul>
<p><strong>Formulating Optimisation Problems</strong></p>
<ol>
<li>Design variables represent a candidate solution</li>
<li>define the search space of candidate solutions</li>
<li>constraints which define solution feasibility</li>
<li>Objective function defines the quality of a solution</li>
<li>maximise or minimise</li>
</ol>
<p>example : TSP</p>
<ol>
<li>Variables :sequence x containing N cities to be visited.</li>
<li>Space : all possible sequences of cities.</li>
<li>Constraints : <ul>
<li>each city must appear once and only once in x;</li>
<li>salesman must return to the city of origin.</li>
</ul>
</li>
<li>Objective function : total_distance(x) = sum of distances between consecutive cities + distance from last city to origin to be minimised + number of missed cities <em> MAX + number of duplicated cities </em> MAX</li>
<li>To be minimised</li>
</ol>
<p>NOTE : Optimisation algorithm can be more efficient than tree-based search algorithms.</p>
<p><strong>Hill-Climbing</strong><br>Algorithm :</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">current_solution = generate initial candidate solution randomly.</span><br><span class="line"></span><br><span class="line">reapeat &#123;</span><br><span class="line">  <span class="function">generate neighbor <span class="title">solutions</span><span class="params">(differ from current solution by a single element)</span></span></span><br><span class="line"><span class="function">  best_neigbor </span>= <span class="function">get highest quality <span class="title">neigbor</span> <span class="params">(&gt;= <span class="number">0</span> 个候选解)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">  <span class="title">if</span><span class="params">(quality(best_neighbor)</span> &lt;</span>= quality(current_solution))&#123;</span><br><span class="line">      <span class="keyword">return</span> current_solution;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> best_neighbor;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>NOTE : Real world problems will frequently have more than one neighbour for each candidate solution.</p>
<p><strong>Designing Representation, Initialisation and Neighbourhood Operators</strong><br>In order to deal with constraints in a problem, two approaches are provided :</p>
<ol>
<li>construct representation, initialisation and neighbourhood operation</li>
<li>add panelties for each constraints in objective function</li>
</ol>
<p>Representation : what data structure we use, how to store a solution<br>Initialisation : how to generate a solution<br>Neighbourhood operator : how we generate a neighbor solution with single element difference</p>
<p>Example : TSP<br>constraints : each city must appear once and only once in x;<br>                  salesman must return to the city of origin.<br>Strategies  :<br>1) Representation :</p>
<ul>
<li>City[4] array [A][B][C][D][E] A</li>
</ul>
<p>2) Initialisation :</p>
<ul>
<li>unvisited_Set = {A,B,C,D,E}</li>
<li>pick k cities from unvisited_Set without duplication</li>
</ul>
<p>3) Neighbourhood operator :</p>
<ul>
<li>reverse the path between cities</li>
<li>A B C D E between B and E –&gt; A B D C E between D and E –&gt; B A D C E</li>
</ul>
<p><strong>Software module clustering</strong><br>design variable : a list of N modules, each has a list of M units.<br>define search space : all possible grouping<br>constraints : N/A (maybe within a module there is a limit of units)<br>objective function : f(M) = Σ(for all n) connections in module n - penalty * connections to other modules + b<br>to be maximised<br>&nbsp;<br>1) Representation :</p>
<ul>
<li>represent each module as a java class</li>
<li>an array to add units</li>
<li>a number to store connections between units</li>
<li>a number to store connections between modules</li>
</ul>
<p>2) Initialisation :</p>
<ul>
<li>generate N modules as an object array</li>
<li>randomly feed m units in each module (m &gt;= 0)</li>
<li>based on units, randomly generate connections between units and between modules.</li>
</ul>
<p>3) Neighbourhood operator :</p>
<ul>
<li>a module has a method to transfer one single random unit to another module</li>
<li>this will remove the unit connections to all other units and unit of other module</li>
</ul>
<p><strong>Advantages and Disadvantages</strong><br>Disadvantage :</p>
<ol>
<li>greedy local search</li>
<li>may not be able to find global optimum.</li>
</ol>
<h2 id="Simulated-annealing-模拟退火"><a href="#Simulated-annealing-模拟退火" class="headerlink" title="Simulated annealing(模拟退火)"></a>Simulated annealing(模拟退火)</h2><p><strong>Simulated annealing</strong><br>Algorithem :</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> </span>&#123;</span><br><span class="line">    current_solution = generate initial candidate solution randomly.</span><br><span class="line">    T = Tmax;</span><br><span class="line">    Tmin = <span class="number">0.1f</span>;</span><br><span class="line"></span><br><span class="line">    reapeat(T &lt;= Tmin || current_solution stops changing)&#123;</span><br><span class="line">        <span class="function">generate neighbor <span class="title">solutions</span><span class="params">(differ from current solution by a single element)</span></span></span><br><span class="line"><span class="function">        rand_neighbor </span>= get random neighbor of current_solution</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(quality(best_neighbor) &lt;= quality(current_solution))&#123;</span><br><span class="line">            <span class="keyword">if</span>(within Probability e^(dE)/T) &#123;</span><br><span class="line">            current_solution = best_neighbor;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> current_solution;</span><br><span class="line">      </span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> best_neighbor;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        T -= some_factor;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function">method <span class="title">dE</span><span class="params">(rand_neighbor, current_solution)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> quality(rand_neighbor) - quality(current_solution);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>from above, dE is ALWAYS between 0 and 1 as neighbor solution is smaller than current solution inside the if condition. And there are some cases we need to clarify :</p>
<ol>
<li>if neighbor solution is really bad, then the probability decreases a lot.</li>
<li>with the decreasing T, (dE/T) is getting smaller, as a result the probability is going down.</li>
</ol>
<p>from these facts, at the begining SA is more likely to jump out a local optimum. With the T decreasing, it behaves more like hill climbing as the probabilty is quite low.<br>&nbsp;<br>NOTES :</p>
<ul>
<li>SA is not guaranteed to find the optimum in a reasonable amount of time</li>
<li>if we leave SA to run forever, it is guaranteed to find an optimal solution,depending on the schedule used.</li>
<li>Frequently it is able to find (a) good solutions.</li>
</ul>
<p><strong>Applications</strong><br>Very-Large-Scale-Integration (VLSI)<br>&nbsp;<br>Design variables :<br>candidate solution z = {(x1, y1), (x2, y2) … (xN, YN)}<br>where N is the number of transistors in the set<br>&nbsp;<br>Search space : all coordinates on the chip<br>&nbsp;<br>Objective function :<br>f(z) = a <em> chip area + w </em> wiring length + c * congestion + penalty<br>where a + w + c = 1<br>penalty : related to x and y which should be in the coordinate system.<br>&nbsp;<br>Constraints :<br>for all x and y, x and y are in the coordinate system.<br>&nbsp;<br>To be minimised</p>
<p><strong>Advantages &amp;&amp; Disadvantages</strong><br>Adv : this algorithm is able to jump out from a local optimum to a global optimum</p>
<p><strong>Code Src</strong><br><a href="https://github.com/jwyhhh123/simulated-annealing">https://github.com/jwyhhh123/simulated-annealing</a></p>
<h2 id="Introduction-to-machine-learning-and-k-NN-聚类算法"><a href="#Introduction-to-machine-learning-and-k-NN-聚类算法" class="headerlink" title="Introduction to machine learning and k-NN(聚类算法)"></a>Introduction to machine learning and k-NN(聚类算法)</h2><p><strong>Introduction to machine learning</strong><br>def : machine learning focuses on studying and developing computational models capable of improving their performance with experience and acquiring knowledge on their own.<br>&nbsp;<br>types of learning :</p>
<ol>
<li>supervised learning (监督学习)</li>
<li>reinforcement learning (强化学习)</li>
<li>unsupervised learning</li>
<li>semi-supervised learning</li>
</ol>
<p><strong>Introduction to supervised learning</strong><br>Predicative tasks : based on existing training data, we are aiming to learn models which are able to make predictions for new data.<br>&nbsp;<br>Components :</p>
<ol>
<li>training data</li>
<li>machine learning algorithm</li>
<li>predictive model</li>
</ol>
<p>Data format : (x, y)</p>
<ul>
<li>x the input attributes : x = (x1, x2, x3 …, xn)</li>
<li>y the output attributes : we focus on ONE output.</li>
</ul>
<p>Example :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">    x1    x2    x3      y</span><br><span class="line"></span><br><span class="line">/  x11   x12   x13  |   y1  \</span><br><span class="line">|  x21   x22   x23  |   y2  |    // matrix representation</span><br><span class="line">\  x31   x32   x33  |   y3  /</span><br></pre></td></tr></table></figure>
<p>Types of input attributes :</p>
<ul>
<li>Numerical   : 123</li>
<li>Ordinal     : {low, medium, high}</li>
<li>Categorical : {UK, CHN, US}</li>
</ul>
<p><strong>Classification &amp;&amp; Regression</strong><br>Classification :</p>
<ul>
<li>the output attributes are categories/ classes</li>
<li>the learning model is called : Classifier</li>
</ul>
<p>Regression :</p>
<ul>
<li>the output attributes are numerical values</li>
</ul>
<p>NOTE : k-nn is dealing with classification and regression.</p>
<p><strong>Example application</strong><br>1) Estimation of the effort required to develop a software project.</p>
<ul>
<li>input  : programming language; team expertise, estimated size, development type…</li>
<li>output : person-hours / months<br>2) Credit card approval </li>
<li>input  : age; salary; number of years holding a bank account…</li>
<li>output : yes / no<br>3) Breast cancer prediction</li>
<li>input  : clump thickness; uniformity of cell size, cell thickness…</li>
<li>output : yes / no</li>
</ul>
<p>NOTE : 监督学习实际上是给当前模型输入已知的数据然后来分析新的数据。</p>
<p><strong>K-NN (K-Nearest Neighbours)</strong><br>k : how many neighbors to choose for classification.<br>&nbsp;<br>classification problems : the test data (x,?) is classified to the majority among the output attributes of neighbour training data.<br>&nbsp;<br>Numerical problems      : the test data is computed by the average among the output attributes of neighbour training data.<br>&nbsp;<br>Algorithm :</p>
<ol>
<li>give the input attributes of a new data (x,?) that we want to predict </li>
<li>give k, the neighbors we are looking for.</li>
<li>For each training example (x’, y’) :<ul>
<li>compute distance(x,x’) = root{ Σ(for all i) sum[(xi - x’(i))^2] }</li>
<li>sort all of the distances computed in ascending order by a data structure</li>
<li>return first k data as the nearest neighbors</li>
</ul>
</li>
<li>return either the majority vote or the average.</li>
</ol>
<p><strong>Normalisation of numeric input attributes</strong><br>Problem : Different numeric input attributes may have different scales. It will affect the Euclidean distance. for example, if x1 is in [0, 10] and x2 is in [100, 10000], then x2 will influence the distance more.</p>
<p>Solution : BEFORE starting training, we need to normalise all input attributes between 0 and 1 :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">                   xi - min(i)</span><br><span class="line">normalised(xi) = --------------- </span><br><span class="line">                 max(i) - min(i)</span><br><span class="line"></span><br><span class="line">min(i)  = the minimum among all x(i)</span><br><span class="line">maxi(i) = the maximum among all x(i)</span><br></pre></td></tr></table></figure>
<p>NOTES : </p>
<ul>
<li>if new training data are loaded, then we need to take them into consider as well. We then require to de-normalise and re-normalise the data (including test data).</li>
<li>this process is to minimize errors.</li>
</ul>
<p><strong>Advantages &amp;&amp; Disadvantages</strong><br>Adv : </p>
<ol>
<li>training is simple and fast, just normalise and store the training data</li>
</ol>
<p>Disadv :</p>
<ol>
<li>require large memory space to store all data</li>
<li>making prediction is slow, we have to sort the whole data set in order to find k neighbors. (the algorithm is slow)</li>
</ol>
<p><strong>Application</strong></p>
<ul>
<li>program effort estimation</li>
<li>movies classification</li>
</ul>
<p><strong>Web Rsc</strong><br>&nbsp; <a href="https://blog.csdn.net/saltriver/article/details/52502253" target="_blank" rel="noopener">https://blog.csdn.net/saltriver/article/details/52502253</a></p>
<h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><p><strong>Bayes theorem</strong><br>P(f, c) = P(f) <em> P(c|f) = P(c) </em> P(f|c)<br>where <code>f</code> is input attributes and <code>c</code> is the output we want ot predict.<br>&nbsp;<br>accordingly, prediction of c with f is :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">         P(c) * P(f|c)</span><br><span class="line">P(c|f) = --------------</span><br><span class="line">              P(f)</span><br></pre></td></tr></table></figure>
<p>NOTES: </p>
<ul>
<li>input attributes 和 output attributes 可以相互转换, 基于问题。</li>
<li>P(c|f) : c out of f</li>
<li>P(f|c) : f out of c</li>
</ul>
<p><strong>Naive Bayes</strong><br>to normalise,</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">P(c|f) = α * P(c) * P(f|c)</span><br><span class="line">       = α * P(c) * P(f1, ..., fn|c)</span><br><span class="line">       = α * P(c) * P(f1|c) * P(f2|c) *... P(fn|c)</span><br><span class="line">       = α * P(c) * Π(for all n) P(fi|c)</span><br><span class="line"></span><br><span class="line">     1</span><br><span class="line">α = --- </span><br><span class="line">     β</span><br><span class="line"></span><br><span class="line">β = Σ(for all c) P(c) * P(f|c)</span><br><span class="line">  = Σ(for all c) P(c) * Π(for all n) P(fi|c)</span><br></pre></td></tr></table></figure>
<p>NOTE : <code>f</code> represents f1, … fn, for all n input attributes</p>
<p>in Naive Bayes, each input attribute is conditionally independent</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">P(x1 | x2, y) = P(x1 | y)   // x1, x2 are input attributes and y is output attribute</span><br></pre></td></tr></table></figure>
<p>Procedure :</p>
<ol>
<li>create table of attributes : for each input attribute and its possible values</li>
<li>apply Laplace Smoothing</li>
<li>build the model of frequency tables(with and without Laplace Smoothing)</li>
<li>computation based on formulas given.</li>
</ol>
<p>NOTE : I think there is a similarity between algorithm and model.</p>
<p><strong>Laplace Smoothing</strong><br>Procedure :</p>
<ol>
<li>add 1 to each of the frequency cells and use that when calculating <code>P(fi|c)</code></li>
<li>when calculate P(c), we MUST use the original frequency to calculate it.</li>
</ol>
<p><strong>Markov chains</strong><br>Frequency tables can also be used to compute the probability of switching between states in random processes.<br>&nbsp;<br>Markov Chain is a probabilistic automata :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">      __                      __</span><br><span class="line">75%  |  |         25%        |  |  50%</span><br><span class="line">     -[sunny] &lt;--------&gt; [rainy]-</span><br><span class="line">                  50%</span><br></pre></td></tr></table></figure>
<p><strong>Categorical attributes</strong><br>the frequency tables are created by counting occurrences of each possible input and output attribute value among the examples in the training set.</p>
<p><strong>Numeric attributes:</strong><br>Probability density functions by using Gaussian distribution N(μ, σ^2) :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">distribution of c being given μ, σ^2</span><br><span class="line">                  1                 -(f - μ)^2</span><br><span class="line">P(fi|c) = ------------------ * e^(--------------)    // π  ≈ 3.14159, e ≈ 2.71828</span><br><span class="line">           root(2 * σ^2 * π)          2 * σ2</span><br><span class="line"></span><br><span class="line">μ   : mean of input attributes being such c = X</span><br><span class="line"></span><br><span class="line">         1</span><br><span class="line">σ^2 : ------- * Σ(for each value being such c) [valuei - μ]^2</span><br><span class="line">      n(value)</span><br></pre></td></tr></table></figure>
<p>NOTES :</p>
<ul>
<li>we need to create density function for each c that we want to predict. for example yes/no has 2 f’s; low/medium/high has 3 f’s.</li>
<li>we compute P(fi|c) by feeding variable f into the density function.</li>
</ul>
<p><strong>Advantages &amp;&amp; disadvantages</strong><br>Advantages :</p>
<ul>
<li>training is fast, only one pass through the data</li>
<li>relative probabilities are suitable for many predictions for the given data</li>
</ul>
<p>Disadvantages :</p>
<ul>
<li>we assume that inputs are conditional independent, this may influence the probability precision</li>
<li>we assume a certain probability distribution for numerical inputs. This means Naive Bayes does not work very well for regression</li>
</ul>
<p><strong>Applications</strong><br>text categorisation : spam mails filtering<br>medical diagnosis<br>software defect prediction(软件缺陷预测)</p>
<p><strong>Web Rsc</strong><br><a href="https://zh.wikipedia.org/wiki/朴素贝叶斯分类器" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/朴素贝叶斯分类器</a></p>
<h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p><strong>Classification</strong><br>def : classify observations into different classes</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y</span><br><span class="line"> |  .                       </span><br><span class="line"> |    .         </span><br><span class="line"> |      .   flue             </span><br><span class="line"> |        .</span><br><span class="line"> | no flue  .     </span><br><span class="line"> |            .</span><br><span class="line"> ----------------------&gt; x</span><br></pre></td></tr></table></figure>
<p><strong>Regression</strong><br>def : given some observations, find a trend of the given data</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y</span><br><span class="line"> |          * .              </span><br><span class="line"> |          .               </span><br><span class="line"> |    *   .  *               </span><br><span class="line"> |     .</span><br><span class="line"> |  *.  *   </span><br><span class="line"> | .            </span><br><span class="line"> ----------------------&gt; x</span><br></pre></td></tr></table></figure>
<p><strong>Least square</strong><br>two ways to formulate badness</p>
<ul>
<li>L1 : Sum of absolute values of distances’ error</li>
<li>L2 : Sum of squares of distances’ error</li>
</ul>
<p>We choose L2 to evaluate our w’s because it punish points which are far away from the best fit.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">L2 = ∑(for each i) (yi - hw(xi))2</span><br></pre></td></tr></table></figure>
<p><strong>gradient descent(univariant)</strong><br>Algorithms : example of y = w0 + w1 * x</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (not converging)&#123;</span><br><span class="line">    <span class="keyword">for</span> each training data j &#123;</span><br><span class="line">        update w1 using loss on j:  w1 = w1 + α * (yj - h(xj)) * xj </span><br><span class="line">        update w0 using loss on j:  w0 = w0 + α * (yj - h(xj))</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>NOTE : univariate means ONLY contain x, x^n; multivariate means contain x1, x2 …</p>
<p><strong>Web src</strong><br><a href="https://zh.wikipedia.org/wiki/最小二乘法" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/最小二乘法</a><br><a href="https://www.zhihu.com/question/37031188" target="_blank" rel="noopener">https://www.zhihu.com/question/37031188</a>)</p>
<p><strong>Linear regression</strong></p>
<ol>
<li>hypothesis h(x) = w0 <em> x0 + w1 </em> x1 + … + wn <em> xn = w^T </em> x  // assume x(0) = 1</li>
<li>cost function: cost = Σ(for each j) (y(j) - hw(x(j)))^2</li>
<li>(partial) differential(w0 w1) : <ul>
<li>univariate  (wn = ?)</li>
<li>multivariate (wn = ?)</li>
</ul>
</li>
</ol>
<h2 id="Logistic-regression"><a href="#Logistic-regression" class="headerlink" title="Logistic regression"></a>Logistic regression</h2><ol>
<li>hypothesis h(x) = sigmoid function: g(z) = 1/(1 + e^(-z)) where z = w^T * x</li>
<li>cost function: cost = -1/m * Σ[ y(i)log(hw(x(i))) + (1-y(i))(log(1 - hw(x(i)))]</li>
<li>(partial) differential : wi = wi + α <em> (y(j) - hw(x(j))) </em> x(i)</li>
<li>decision boundary </li>
</ol>
<p>hypothesis h(x) gives the probability between 0 and 1. In general we predict :</p>
<ul>
<li>y = 1 if h(x) &gt;  0.5</li>
<li>y = 0 if h(x) &lt;= 0.5</li>
</ul>
<p><strong>linear separable</strong><br>Similar to linear regression, the logistic regression decision boundary can be made more complex(not straight line) by adding non-linear terms.<br>&nbsp;<br>linear separable : when the decision boundary is a straight line.</p>
<p><strong>Cost function</strong><br>Our cost function for logistic regression is different because it is not convex function :<br>cost = -1/m <em> Σ [y(i) </em> log(hw(x(i))) + (1-y(i)) * (log(1 - hw(x(i)))]<br><code>m</code> : m data<br>&nbsp;<br>NOTE : 代价函数不是凸函数; 凸函数，有全局最优解</p>
<p><strong>Training data &amp; Test data</strong><br>Training data:</p>
<ul>
<li>provides a way to form a model for new data</li>
<li>the model is based on previous data set<br>&nbsp;<br>Data format :</li>
</ul>
<ol>
<li>(x(j), y(j)) for data j</li>
<li>(x1(j),x2(j),x3(j), …,y(j)) for data j with inputs 1,2,3 </li>
</ol>
<p>Test data: by using the model we can make prediction for new data (x, ?)</p>
<p><strong>Overfitting and Underfitting</strong><br>overfitting : </p>
<ul>
<li>result of a model that is more complex than it required.</li>
<li>fit exactly to training data and will perform badly on test data</li>
</ul>
<p>underfitting :</p>
<p><strong>Model Complexity</strong><br>complexity of a model is determined by how many nonlinear terms there are in a training model.<br>&nbsp;<br>NOTE : if there are many x^n, then it causes overfitting.</p>
<p><strong>Regularisation</strong><br>Cost = Cost + ƛ <em> Σ(for all w’s) (wi)^2            - L2 Regularisation<br>wi   = wi + α </em> (y(j) - hw(x(j))) <em> x(i) + ƛ </em> wi<br>w0   = w0 + α * (y(j) - hw(x(j)))<br><code>ƛ</code>：how much to penalise complex terms.<br>&nbsp;<br>It can be shown that smaller values of w will result in the “disappearance” of higher polynomials.<br>&nbsp;<br>NOTE : increase ƛ leads to higher regularisation, less model complexity and less overfitting.(reduce the effect from higher order polynomials)</p>
<p><strong>Limitations</strong><br>If the hypothesis function is non-linear and multivariate, where the number of independent variables is large, the number of “weights” required to be updated would be extremely large (impractical).</p>
<p><strong>Web Rsc</strong><br><a href="https://blog.csdn.net/walilk/article/details/50978864" target="_blank" rel="noopener">https://blog.csdn.net/walilk/article/details/50978864</a><br><a href="https://zhuanlan.zhihu.com/p/28408516" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28408516</a><br><a href="https://blog.csdn.net/nwpuwyk/article/details/36065153" target="_blank" rel="noopener">https://blog.csdn.net/nwpuwyk/article/details/36065153</a><br>&nbsp;<br>for all information  : <a href="https://scruel.gitee.io/ml-andrewng-notes/" target="_blank" rel="noopener">https://scruel.gitee.io/ml-andrewng-notes/</a><br>Andrew ng online lec : <a href="https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN" target="_blank" rel="noopener">https://www.youtube.com/playlist?list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN</a></p>
<h2 id="Revisit"><a href="#Revisit" class="headerlink" title="Revisit"></a>Revisit</h2><p><em>For some</em><br><code>hw(x) = w0 + w1x1 + w2x2 + w3x3 ...</code></p>
<p><em>Your updates are:</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w0 += ... * 1</span><br><span class="line">w1 += ... * x1</span><br><span class="line">w2 += ... * x2</span><br><span class="line">w3 += ... * x3</span><br></pre></td></tr></table></figure>
<p><em>For some</em><br><code>hw(x) = w0 + w1x + w2x^2</code></p>
<p><em>Your updates are:</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w0 += ... * 1</span><br><span class="line">w1 += ... * x</span><br><span class="line">w2 += ... * x^2</span><br></pre></td></tr></table></figure>
<p><em>For some</em><br><code>hw(x) = w0 + w1x1 + w2x2 + w3x1x2</code></p>
<p>Your updates are:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w0 += ... * 1</span><br><span class="line">w1 += ... * x1</span><br><span class="line">w2 += ... * x2</span><br><span class="line">w3 += ... * x1 * x2</span><br></pre></td></tr></table></figure>
<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p><strong>Reason we use neural network</strong><br>We found a logistic regression has a lack of classifying non-linear separable data when the number of variables increases as well as weights increasing. Instead, we use neural network to create multilayered variables in order to hold more than one logistic regressions.<br>&nbsp;<br>NOTE : 神经网络是逻辑回归的加强版</p>
<p><strong>Representation:</strong><br>linear logits:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">      w0</span><br><span class="line">[1]  -----.</span><br><span class="line">      w1   \</span><br><span class="line">[x1] --------&gt; [ g(z) ] -----&gt; hw(x)</span><br><span class="line">      w2   /      :</span><br><span class="line">[x2] -----.       :</span><br><span class="line">                  sigmoid function</span><br></pre></td></tr></table></figure>
<p>Bias : 1 * w0 which separates the result of 1 and 0</p>
<p>we evaluate sigmoid function g(z):</p>
<ul>
<li>if z &lt; -4, then g(z) = 0</li>
<li>if z &gt;  4, then g(z) = 1</li>
</ul>
<p><strong>Forward propagation</strong><br>def : the process of calculating the inputs of subsequent layers until we reach the final input</p>
<p><strong>Backpropagation</strong><br>def : an algorithm to find the gradient for neural network.<br>&nbsp;<br>Backpropagation is used to calculate the gradient(difference in weights); the gradient is calculated for the purpose of updating the weights of all logits.</p>
<p><strong>one-vs-all</strong><br>def : logistic regression can be used on multiple classes by using one-vs-all.<br>&nbsp;<br>NOTE : for example we have a logistic regression to classify cat, dog, rat; another example, we can apply this on human facial identification which tells you who is the person from a given picture.</p>
<p><strong>Web Rsc</strong><br>&nbsp; <a href="http://www.ruanyifeng.com/blog/2017/07/neural-network.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2017/07/neural-network.html</a><br>&nbsp; <a href="https://www.youtube.com/watch?v=oI1eJa-U" target="_blank" rel="noopener">https://www.youtube.com/watch?v=oI1eJa-U</a> WNU<br>&nbsp; <a href="https://www.jianshu.com/p/4cf34bf158a1" target="_blank" rel="noopener">https://www.jianshu.com/p/4cf34bf158a1</a></p>
<p><strong>Cost function</strong><br>cost = Σ(for each j) (expected - predicted)^2</p>
<p><strong>Regularisation</strong><br>Neural networks, like all non-linear models can overfit training data, there are three approaches :</p>
<ol>
<li>include regularisation for each logistic unit</li>
<li>Dropout : ignore certain nodes(typically half) when traing the model</li>
<li>reduce network depth : reduce the complexity of hypothesis h(x)</li>
</ol>
<p><strong>Activation Function</strong><br>def: function that used on the weighted sum of input<br>&nbsp;<br>NOTE : we learnt sigmoid function as an activation function in neural network, but there are others.</p>
<p><strong>Hyperparameters</strong><br>Neural Networks (and to a lesser extent Logistic Regression) require several parameters that can be used to “tune” how the network trains.<br>&nbsp;<br>Hyperparameter tuning : the process of picking the correct combination of hyperparameters. This is extremely expensive in terms of time and computational resources.<br>&nbsp;<br>Grid Search : the process of systematically going through all possible combination of the hyperparameters to find the best<br>&nbsp;<br><strong>“Overfiting”</strong><br>def : a combination of hyperparameters which works well for training data and does not work for any other data.<br>&nbsp;<br>to deal with this, we split our data into three parts :</p>
<ol>
<li>training set</li>
<li>development set : used for hyperparameter tuning</li>
<li>test set</li>
</ol>
<p>proportions of dividing :<br>80 : 10 : 10<br>60 : 20 : 20</p>
<p><strong>Implementations</strong><br>Fraud Prevention<br>Facial identification(人脸识别)</p>
<h2 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h2><p><strong>Agents</strong><br>In reinforcement learning, an agent is required to reach an end state in an environment<br>&nbsp;<br>Example :</p>
<ul>
<li>Agent       : game</li>
<li>End state   : win</li>
<li>Environment : board</li>
</ul>
<p>In order to reach the outcome we expected, we introduce reward :</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//for some [i] action</span></span><br><span class="line"><span class="keyword">switch</span>(i)&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="number">1</span>: goal -= <span class="number">100000</span>  <span class="comment">// large panalty</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">2</span>: goal -= <span class="number">10</span>      <span class="comment">// small penalty</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">3</span>: goal += <span class="number">100000</span>  <span class="comment">// large reward</span></span><br><span class="line">  <span class="keyword">case</span> <span class="number">4</span>: goal += <span class="number">10</span>      <span class="comment">// small reward</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Things to know :</p>
<ul>
<li>reward and penalty are based on how they act in an environment</li>
<li>an agent aims to maximise positive reward</li>
<li>agent uses this to train a model(e.g. neural network)</li>
</ul>
<p>Environment should be fully observable or partially Observable.</p>
<p><strong>Web Rsc</strong><br>&nbsp; <a href="https://www.youtube.com/watch?v=4MlZncsh" target="_blank" rel="noopener">https://www.youtube.com/watch?v=4MlZncsh</a> y1Q<br>&nbsp; <a href="https://www.youtube.com/watch?v=cUTMhmVh" target="_blank" rel="noopener">https://www.youtube.com/watch?v=cUTMhmVh</a> 1qs</p>

  </div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Robotics"><span class="toc-number">1.</span> <span class="toc-text">Robotics:</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Robot-sensors"><span class="toc-number">1.1.</span> <span class="toc-text">Robot sensors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bobot-Motors"><span class="toc-number">1.2.</span> <span class="toc-text">Bobot Motors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Arbitrator"><span class="toc-number">1.3.</span> <span class="toc-text">Arbitrator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Color-Sensor"><span class="toc-number">1.4.</span> <span class="toc-text">Color Sensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Navigator"><span class="toc-number">1.5.</span> <span class="toc-text">Navigator</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mapping"><span class="toc-number">1.6.</span> <span class="toc-text">Mapping</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Robot-wifi-connection-amp-assignment"><span class="toc-number">1.7.</span> <span class="toc-text">Robot wifi connection &amp; assignment</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bluetooth-connection"><span class="toc-number">1.8.</span> <span class="toc-text">Bluetooth connection</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Theory-Part"><span class="toc-number">2.</span> <span class="toc-text">Theory Part:</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Swarm-intelligence-and-firefly-synchronization"><span class="toc-number">2.1.</span> <span class="toc-text">Swarm intelligence and firefly synchronization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Uninformed-Search"><span class="toc-number">2.2.</span> <span class="toc-text">Uninformed Search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#A-star-Search"><span class="toc-number">2.3.</span> <span class="toc-text">A star Search</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Hill-Climbing"><span class="toc-number">2.4.</span> <span class="toc-text">Hill Climbing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Simulated-annealing-模拟退火"><span class="toc-number">2.5.</span> <span class="toc-text">Simulated annealing(模拟退火)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction-to-machine-learning-and-k-NN-聚类算法"><span class="toc-number">2.6.</span> <span class="toc-text">Introduction to machine learning and k-NN(聚类算法)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Naive-Bayes"><span class="toc-number">2.7.</span> <span class="toc-text">Naive Bayes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression"><span class="toc-number">2.8.</span> <span class="toc-text">Linear Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-regression"><span class="toc-number">2.9.</span> <span class="toc-text">Logistic regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Revisit"><span class="toc-number">2.10.</span> <span class="toc-text">Revisit</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Network"><span class="toc-number">2.11.</span> <span class="toc-text">Neural Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reinforcement-learning"><span class="toc-number">2.12.</span> <span class="toc-text">Reinforcement learning</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&text=Log.AI"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&is_video=false&description=Log.AI"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Log.AI&body=Check out this article: https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&title=Log.AI"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://github.com/jwyhhh123/jwyhhh123.github.io.git/2019/02/19/Log-AI/&name=Log.AI&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 Wenye Jin
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'jwy';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>


</body>
</html>
